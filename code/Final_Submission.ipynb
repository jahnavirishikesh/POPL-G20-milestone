{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Kaggle - python"
      ],
      "metadata": {
        "id": "JS8eF-LB3mdU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StkePG-y3jvi"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "_GGM40gz3rYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d crmercado/tweets-blogs-news-swiftkey-dataset-4million/"
      ],
      "metadata": {
        "id": "l7S_0beF3s50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir kaggle_data\n",
        "! unzip \"/content/tweets-blogs-news-swiftkey-dataset-4million.zip\" -d kaggle_data"
      ],
      "metadata": {
        "id": "zHR8pd8Z3uY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pyro Installation"
      ],
      "metadata": {
        "id": "J_2qnbI84lRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchvision\n",
        "!pip3 install pyro-ppl"
      ],
      "metadata": {
        "id": "wV140KSJ4px2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.poutine as poutine\n",
        "from torch.distributions import constraints\n",
        "from pyro.nn import PyroModule, PyroParam, PyroSample\n",
        "from pyro.nn.module import to_pyro_module_\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.infer.autoguide import AutoNormal\n",
        "from pyro.optim import Adam"
      ],
      "metadata": {
        "id": "3fUkoCBV4sb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning\n"
      ],
      "metadata": {
        "id": "-U8dsb8-34Wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "59caNkIe31Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stop word analysis\n",
        "nltk.download('stopwords')\n",
        "# Create a set of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to remove stop words from a sentence\n",
        "def remove_stop_words(sentence):\n",
        "  # Split the sentence into individual words\n",
        "  words = sentence.split()\n",
        "\n",
        "  # Use a list comprehension to remove stop words\n",
        "  filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "  # Join the filtered words back into a sentence\n",
        "  return ' '.join(filtered_words)"
      ],
      "metadata": {
        "id": "fgl192vX3_V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "file=open(\"/content/kaggle_data/final/en_US/en_US.news.txt\",\"r\")\n",
        "file.seek(0)\n",
        "l1 = file.readlines()\n",
        "print(l1[0])\n",
        "res = []\n",
        "\n",
        "#only 10000 paras allowed\n",
        "for para in l1[:1000]:\n",
        "  temp = para\n",
        "  #temp = remove_stop_words(para)\n",
        "  temp = temp.lower()\n",
        "  res = res + [x for x in re.split(\"[//.|//!|//?]\", temp) if x!=\"\"]\n",
        "print(res)\n",
        "text_data = res\n",
        "#setnences made above\n",
        "\n",
        "# Remove special characters and words between them using regex\n",
        "text_data = [re.sub(r\"\\[.*?\\]\", \"\", text) for text in text_data]\n",
        "\n",
        "# Remove words not in the English alphabet\n",
        "english_alphabet = set(string.ascii_lowercase)\n",
        "text_data = [' '.join([word for word in text.split() if all(char in english_alphabet for char in word)]) for text in text_data]\n",
        "\n",
        "# Remove leading/trailing whitespaces\n",
        "text_data =  text_data\n",
        "\n",
        "# Remove empty sentences\n",
        "# text_data =\n",
        "\n",
        "# Create a DataFrame with the cleaned text data\n",
        "df = pd.DataFrame({\"Text\": text_data})\n",
        "\n",
        "# Save the cleaned text data to a CSV file\n",
        "output_path = \"output.csv\"\n",
        "# Set index=False to exclude the index column in the output\n",
        "df.to_csv(output_path, index=False)\n",
        "print (len(df))\n",
        "print(\"Text data cleaned and saved to:\", output_path)"
      ],
      "metadata": {
        "id": "B1AsAKoj4Ghx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classes\n"
      ],
      "metadata": {
        "id": "NlGTFUMD4Y_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from collections import Counter\n",
        "from torch import nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "SHl_nWCM4Wop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#class TextDataset(torch.utils.data.Dataset):\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "\tdef __init__(self, args):\n",
        "\t\tself.args = args\n",
        "\t\tself.words = self.load_words()\n",
        "\t\tself.unique_words = self.get_unique_words()\n",
        "\n",
        "\t\tself.index_to_word = {index: word for index, word in enumerate(self.unique_words)}\n",
        "\t\tself.word_to_index = {word: index for index, word in enumerate(self.unique_words)}\n",
        "\n",
        "\t\tself.word_indexes = [self.word_to_index[w] for w in self.words]\n",
        "\n",
        "\tdef load_words(self):\n",
        "\t\ttrain_df = pd.read_csv('/content/output.csv')\n",
        "\t\ttext = train_df['Text'].str.cat(sep=' ')\n",
        "\t\treturn text.split(' ')\n",
        "\n",
        "\tdef get_unique_words(self):\n",
        "\t\tword_counts = Counter(self.words)\n",
        "\t\treturn sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.word_indexes) - self.args\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\treturn (\n",
        "\t\t\ttorch.tensor(self.word_indexes[index:index + self.args]),\n",
        "\t\t\ttorch.tensor(self.word_indexes[index + 1:index + self.args+ 1])\n",
        "\t\t)"
      ],
      "metadata": {
        "id": "eSkuf-D_4bXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch import nn\n",
        "class LSTMModel(nn.Module):\n",
        "\tdef __init__(self, dataset):\n",
        "\t\tsuper(LSTMModel, self).__init__()\n",
        "\t\tself.lstm_size = 128\n",
        "\t\tself.embedding_dim = 128\n",
        "\t\tself.num_layers = 3\n",
        "\t\tself.n = True\n",
        "\n",
        "\t\tn_vocab = len(dataset.unique_words)\n",
        "\t\tself.embedding = nn.Embedding(\n",
        "\t\t\tnum_embeddings=n_vocab,\n",
        "\t\t\tembedding_dim=self.embedding_dim,\n",
        "\t\t)\n",
        "\t\tself.lstm = nn.LSTM(\n",
        "\t\t\tinput_size=self.embedding_dim,\n",
        "\t\t\thidden_size=self.lstm_size,\n",
        "\t\t\tnum_layers=self.num_layers,\n",
        "\t\t\tdropout=0.2,\n",
        "\t\t)\n",
        "\t\tself.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "\tdef forward(self, x=None, prev_state=None):\n",
        "\t\t#if (self.n == True):\n",
        "\t\t\t# print(prev_state)\n",
        "\t\t\t# print(\"----\")\n",
        "\t\t\t# print(model)\n",
        "\t\tglobal saving_state2\n",
        "\t\tsaving_state2 = prev_state\n",
        "\t\tembed = self.embedding(x)\n",
        "\t\tself.n = False\n",
        "\t\toutput, state = self.lstm(embed, prev_state)\n",
        "\t\t#print(prev_state)\n",
        "\t\tlogits = self.fc(output)\n",
        "\n",
        "\t\treturn logits, state\n",
        "\n",
        "\tdef init_state(self, sequence_length):\n",
        "\t\treturn (\n",
        "\t\t\ttorch.zeros(self.num_layers, sequence_length, self.lstm_size), torch.zeros(self.num_layers, sequence_length, self.lstm_size)\n",
        "\t\t)"
      ],
      "metadata": {
        "id": "J12KjuxZ4dOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YHGi__OZAY2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from collections import Counter\n",
        "import copy\n",
        "saving_state2 = 1\n",
        "# Hyperparameters\n",
        "sequence_length = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.003\n",
        "num_epochs = 20\n",
        "\n",
        "# Create the dataset\n",
        "dataset = TextDataset(sequence_length)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset,\n",
        "\t\t\t\t\t\t\t\t[train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset,\n",
        "\t\t\t\t\tbatch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "\t\t\t\t\t\tbatch_size=batch_size)\n",
        "\n",
        "# Create the model\n",
        "model = LSTMModel(dataset)\n",
        "pyro_model = copy.deepcopy(model)\n",
        "#to_pyro_module_(pyro_model)\n",
        "#to_pyro_module_(model)\n",
        "# print(model)\n",
        "# print(pyro_model)\n",
        "# print(\"----\")\n",
        "# # Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "\tmodel.train()\n",
        "\ttotal_loss = 0.0\n",
        "\n",
        "\tfor batch in train_loader:\n",
        "\t\tinputs, targets = batch\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\thidden = model.init_state(sequence_length)\n",
        "\t\toutputs, _ = model(inputs, hidden)\n",
        "\n",
        "\t\tloss = criterion(outputs.view(-1,\n",
        "\t\t\t\t\tlen(dataset.unique_words)), targets.view(-1))\n",
        "\t\tloss.backward()\n",
        "\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\ttotal_loss += loss.item()\n",
        "\n",
        "\t# Calculate average loss for the epoch\n",
        "\taverage_loss = total_loss / len(train_loader)\n",
        "\n",
        "\t# Print the epoch and average loss\n",
        "\tprint(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "\t# Validation loop\n",
        "\tmodel.eval()\n",
        "\tval_loss = 0.0\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\tfor batch in val_loader:\n",
        "\t\t\tinputs, targets = batch\n",
        "\n",
        "\t\t\thidden = model.init_state(sequence_length)\n",
        "\t\t\toutputs, _ = model(inputs, hidden)\n",
        "\n",
        "\t\t\tloss = criterion(outputs.view(-1,\n",
        "\t\t\t\t\t\t\tlen(dataset.unique_words)), targets.view(-1))\n",
        "\t\t\tval_loss += loss.item()\n",
        "\n",
        "\t# Calculate average validation loss for the epoch\n",
        "\taverage_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "\t# Print the epoch and average validation loss\n",
        "\tprint(f\"Epoch[{epoch+1}/{num_epochs}], Validation Loss: {average_val_loss: .4f}\")\n"
      ],
      "metadata": {
        "id": "pDaBgg_K4kaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pyro execution\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from collections import Counter\n",
        "import copy\n",
        "import pyro.infer.autoguide as autoguide\n",
        "from pyro.infer import SVI, TraceGraph_ELBO\n",
        "import torch.optim as optim\n",
        "sequence_length = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.003\n",
        "num_epochs = 20\n",
        "\n",
        "# Create the dataset\n",
        "dataset = TextDataset(sequence_length)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset,\n",
        "\t\t\t\t\t\t\t\t[train_size, val_size])\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset,\n",
        "\t\t\t\t\tbatch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset,\n",
        "\t\t\t\t\t\tbatch_size=batch_size)\n",
        "\n",
        "# Create the model\n",
        "model = LSTMModel(dataset)\n",
        "to_pyro_module_(model)\n",
        "guide = autoguide.AutoNormal(model)\n",
        "optimizer = Adam({\"lr\": 0.01})\n",
        "#optimizer = torch.optim.SGD\n",
        "scheduler = pyro.optim.ExponentialLR({'optimizer': optimizer, 'optim_args': {'lr': 0.01}, 'gamma': 0.1})\n",
        "svi = SVI(model, guide, scheduler, loss=TraceGraph_ELBO())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\tmodel.train()\n",
        "\ttotal_loss = 0.0\n",
        "\n",
        "\tfor batch in train_loader:\n",
        "\t\tinputs, targets = batch\n",
        "\n",
        "\t\thidden = model.init_state(sequence_length)\n",
        "\t\toutputs, _ = model(inputs, hidden)\n",
        "\n",
        "\t\tloss = criterion(outputs.view(-1,\n",
        "\t\t\t\t\tlen(dataset.unique_words)), targets.view(-1))\n",
        "\t\tloss.backward()\n",
        "\n",
        "\t\t#optimizer.step()\n",
        "\n",
        "\t\ttotal_loss += loss.item()\n",
        "\n",
        "\t# Calculate average loss for the epoch\n",
        "\taverage_loss = total_loss / len(train_loader)\n",
        "\n",
        "\t# Print the epoch and average loss\n",
        "\tprint(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "\t# Validation loop\n",
        "\tmodel.eval()\n",
        "\tval_loss = 0.0\n",
        "\tscheduler.step()\n"
      ],
      "metadata": {
        "id": "EkfqNnFe6IvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input a sentence\n",
        "list_setences = [\"his family is\", \"on his way to\",\"in the end he\"]\n",
        "\n",
        "# Preprocess the input sentence\n",
        "for input_sentence in list_setences:\n",
        "  print(input_sentence)\n",
        "  input_indexes = [dataset.word_to_index[word] for word in input_sentence.split()]\n",
        "  input_tensor = torch.tensor(input_indexes, dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "  # Generate the next word\n",
        "  model.eval()\n",
        "  hidden = model.init_state(len(input_indexes))\n",
        "  #print(hidden)\n",
        "  outputs, _ = model(input_tensor, hidden)\n",
        "  predicted_index = torch.argmax(outputs[0, -1, :]).item()\n",
        "  predicted_word = dataset.index_to_word[predicted_index]\n",
        "\n",
        "  # Print the predicted word\n",
        "  print(\"Input Sentence:\", input_sentence)\n",
        "  print(\"Predicted Next Word python:\", predicted_word)"
      ],
      "metadata": {
        "id": "Ux1yRFlK8er5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}